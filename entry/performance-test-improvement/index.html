<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Redefining Quipper‚Äôs Performance Testing Platform | Ekky Defa Rizkyan</title><meta name=keywords content><meta name=description content="
How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ"><meta name=author content><link rel=canonical href=https://defa-ekky1073.github.io/entry/performance-test-improvement/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://defa-ekky1073.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://defa-ekky1073.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://defa-ekky1073.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://defa-ekky1073.github.io/apple-touch-icon.png><link rel=mask-icon href=https://defa-ekky1073.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://defa-ekky1073.github.io/entry/performance-test-improvement/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/css/custom.css><meta property="og:url" content="https://defa-ekky1073.github.io/entry/performance-test-improvement/"><meta property="og:site_name" content="Ekky Defa Rizkyan"><meta property="og:title" content="Redefining Quipper‚Äôs Performance Testing Platform"><meta property="og:description" content=" How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="entry"><meta property="article:published_time" content="2025-11-10T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-10T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Redefining Quipper‚Äôs Performance Testing Platform"><meta name=twitter:description content="
How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Entries","item":"https://defa-ekky1073.github.io/entry/"},{"@type":"ListItem","position":2,"name":"Redefining Quipper‚Äôs Performance Testing Platform","item":"https://defa-ekky1073.github.io/entry/performance-test-improvement/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Redefining Quipper‚Äôs Performance Testing Platform","name":"Redefining Quipper‚Äôs Performance Testing Platform","description":" How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ","keywords":[],"articleBody":" How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ Performance testing at scale is rarely glamorous. It‚Äôs slow, unpredictable, and often under-invested, until something breaks. When I joined Quipper, our performance-testing workflow was functional but far from efficient. Tests took nearly 2 hours to run, required significant engineering effort, and consumed more infrastructure than we were comfortable with. And as our products grew, the process was increasingly hard to justify.\nThe problem Before the redesign, our performance-testing workflow was slow, expensive, and almost entirely manual. Running a single test felt like assembling a lab experiment from scratch, every time.\nHere‚Äôs how it worked:\n1. Everything was manual Nothing about the workflow was automated. Every test run required a human QA engineer to perform dozens of manual steps just to get things going.\n2. Manual file preparation + upload To start a test, the QA team had to manually prepare and upload all JMeter artifacts, such as:\n.jmx test plans .csv dataset files .test configuration assets These files were uploaded to S3, only to be manually pulled down again later on the servers where the test would run. This workflow was repetitive and time-consuming.\n3. Overreliance on AWS WorkSpaces (and its high cost) The entire test environment ran on AWS WorkSpaces, which is essentially a fully managed virtual desktop (VDI), great for office workers, not so great for distributed load testing.\nFor each performance test:\nQA engineers had to spin up 25 WorkSpace nodes Each had to be started manually And shut down manually Beyond the friction, WorkSpaces was overpriced for this use case.\nüìä Cost Estimation: A Windows WorkSpace (PowerPro bundle) is: ‚Ä¢ $7.25 monthly base fee per instance ‚Ä¢ $1.53/hour usage ‚Ä¢ $1.53/hr √ó 25 instances √ó 2 hours/test = $76.50/run If QA runs the test once per day: ‚Ä¢ $76.50 √ó 30 days = $2,295/month in usage fees ‚Ä¢ Plus $7.25 √ó 25 instances = $181.25 in monthly base fees ‚Ä¢ Total: ~$2,476.25/month If tests run twice per day: ‚Ä¢ $153 √ó 30 days = $4,590/month in usage fees ‚Ä¢ Plus $181.25 in monthly base fees ‚Ä¢ Total: ~$4,771.25/month That's only the compute cost ‚Äî storage, data transfer, and base fees add more. Realistic total costs (including overhead): ‚Ä¢ ~$6,000‚Äì$8,000/month ‚Ä¢ Up to ~$72,000‚Äì$96,000/year And that's only for performance testing. Furthermore:\nWorkSpaces billing rounds hourly If you run 61 minutes, you pay 2 hours Billing confusion + manual shutdowns often led to over-billing QA was limited to working within time windows to keep cost down In Short:\n‚ùó We were spending cloud-desktop money to run distributed load testing and paying for the privilege of doing it manually.\n4. Manual distribution of test artifacts Once WorkSpaces were up, the QA engineer still had to:\nConnect to the first node using AWS Console Download the test assets from S3 Run 15 separate Python files to distribute the test assets to each node If anything failed, the QA had to restart the distribution manually. There was no retry automation, no integrity checking, and no self-healing.\n5. Manual execution of JMeter After distribution, the QA would initiate JMeter from the first instance. This instance acted as the controller, coordinating 24 secondary nodes.\nAny misconfiguration, file mismatch, missing dependency, wrong path, etc., requires starting over.\n6. Manual result upload Once the test finished:\nThe QA exported results Manually uploaded them back to S3 They have to download it manually to their local machine Only after that could anyone analyze the output or generate reports.\n7. Manual shutdown Finally, and critically! All 25 WorkSpaces had to be shut down manually! If someone forgot, WorkSpaces kept charging by the hour. And because of hourly billing rounding, even short mistakes were costly.\nAll of those steps took about 2 hours to complete. This meant:\nLong iteration cycles Low test frequency Higher risk of missed regressions High operational burden In short:\n‚ùó üìâ Performance testing wasn‚Äôt a tool, it was a tax.\nIt‚Äôs wasting available resources: time, money, and man-hours.\nThe Approach Once we identified that the real bottleneck wasn‚Äôt the testing tool, but the process, the goal became simple:\nMake performance testing fast, automated, flexible, and cost-efficient\nInstead of patching the old process, I decided to rebuild it from the ground up using Infrastructure as Code, automation, and lightweight orchestration. 1. Infrastructure Redesign with Terraform + GitHub Actions The first step was to replace AWS WorkSpaces entirely. They were slow, expensive, and ill-suited for distributed automation.\nI migrated the testing infrastructure to AWS EC2, provisioning all resources through Terraform. This gave us a clean, declarative setup where every environment could be built, destroyed, or modified automatically and repeatably.\nTo make the process fully automated and auditable:\nTerraform runs are executed directly through GitHub Actions All infrastructure changes are committed and tracked in our IaC repository Any updates to configuration automatically trigger deployments Every test environment is now ephemeral: created on demand, and destroyed after use.\nEach EC2 instance is pre-configured automatically via user-data scripts, which install:\nJMeter Dependencies (Python, monitoring agents, etc.) Environment-specific configurations This means the QA engineer doesn‚Äôt need to install or configure anything manually, the infrastructure configures itself.\n2. Simplifying Configuration for QA Engineers While Terraform gave us automation, it wasn‚Äôt friendly to non-engineers. So I designed the setup around a single configuration file local.tf. which exposes all the tunable parameters the QA team might need:\n# General Configuration region = \"us-east-1\" jmeter_version = \"5.6.3\" bucket_name = \"performance-test-bucket\" # Only change this if required, otherwise let it be instance_tag = \"performance_test\" # This must be have same value as in python distributor, autimatically set worker and master tag ami_id = data.aws_ami.amazon_linux_2023.id # Amazon Linux 2023 AMI (dynamically fetched) # Change ami_id if you need to change instances OS, but make sure to update the user_data on instance.tf # Worker Configuration instance_count = 2 worker_instance_type = \"c6g.large\" # You can check instance types on https://instances.vantage.sh/ # Master Configuration enable_master = true # Set this to false if you going to use your local machine as master master_instance_type = \"c6g.large\" # You can check instance types on https://instances.vantage.sh/ All critical parameters, such as the number of nodes, instance type, tags, JMeter version can be adjusted from one place. QA engineers don‚Äôt have to touch multiple .tf files or understand Terraform internals.\nThis abstraction turned infrastructure setup into something simple, safe, and self-serve.\nNo SRE intervention needed.\n3. Introducing PyDist ‚Äî the Python Controller Once the infrastructure was automated, I built a Python-based orchestration tool we call PyDist (Python Distributor) as the control layer that manages the entire testing lifecycle. PyDist connects everything into a single, reliable workflow.\nHere‚Äôs what it does:\n1. Configuration Validation Utilizes Boto3 to verify that:\nAll EC2 nodes (master + workers) are running and reachable S3 buckets are accessible Connections between master and worker nodes are healthy 2. File Distribution Automatically uploads all .jmx files, .csv variable files, and configurations to the appropriate nodes. No more manual S3 uploads or remote copy scripts.\n3. Service Control Starts and stops the JMeter service on each EC2 instance, ensuring consistent test environments every time.\n4. Test Execution Runs CLI-based JMeter tests from the master node, coordinating all worker nodes in distributed mode. PyDist also monitors progress and collects test logs in real-time.\n5. Result Management Once tests complete:\nResults are automatically uploaded to S3 PyDist downloads the results to the local machine for analysis It can also re-download past test results if needed for reporting or regression comparison In essence, PyDist replaced dozens of manual steps with a single command and brought order, visibility, and repeatability to the whole process.\n4. Automating Cost Efficiency With the infrastructure and orchestration layers complete, there was one more challenge: idle, unused resources.\nTo prevent unnecessary cloud costs, I created two GitHub Actions workflows:\n1. Auto-Shutdown Workflow Runs every midnight Automatically shuts down all EC2 nodes tagged as performance-testing instances Ensures we never pay for unused instances, even if someone forgets to stop them 2. Cluster Startup Workflow Triggered manually when QA needs to run tests Spins up the entire test cluster automatically QA no longer needs AWS console access or Terraform knowledge, it‚Äôs just a button click This combination gave us full control, safety, and significant cost savings without sacrificing flexibility.\n5. The Outcome By combining Terraform, GitHub Actions, and PyDist, we turned a clunky, manual workflow into a streamlined, automated, and maintainable system.\n‚úÖ No more manual uploads or remote logins ‚úÖ Fully documented and version-controlled infrastructure ‚úÖ Self-service configuration for QA ‚úÖ Automated start/stop + cost control ‚úÖ Reliable, repeatable, fast test runs And the result?\nüöÄ Test cycle time reduced from 2 hours ‚Üí 15 minutes üí∏ ~$60,000/year in infrastructure savings üòÉ Plus unmeasured (but huge) savings in time, manual effort, and frustration. Lesson Learned Rebuilding the performance-testing platform taught me that real reliability isn‚Äôt about tools, it‚Äôs about systems. The old workflow didn‚Äôt fail because JMeter was bad; it failed because the process around it wasn‚Äôt built to scale.\nHere‚Äôs what stood out the most:\n1. Fix the process, not just the tool Automation works best when it removes human dependency. By redesigning the workflow we turned a manual process into a repeatable system.\n2. IaC is communication, not just code Terraform made every change visible, reviewable, and reversible. It replaced tribal knowledge with a shared, documented truth.\n3. Empower, don‚Äôt gatekeep By simplifying configuration for QA, we gave ownership back to the team. They could run, tweak, and iterate without waiting on SREs.\n4. Build cost efficiency into design We stopped relying on people to remember to shut things down. With automation, cost-saving became an inherent behavior, not an afterthought.\n5. Make testing a habit When tests are fast and simple, people run them often. That‚Äôs how reliability becomes part of the culture, not a special event.\nFinal Take What started as a slow, frustrating process became an elegant system that just works. Tests that once took hours now finish in minutes, infrastructure spins up and down on demand, and engineers can focus on insight instead of setup.\nBut the biggest win isn‚Äôt the speed or the savings: it‚Äôs the shift in mindset. Performance testing stopped being a chore and became a shared discipline.\nThank you for taking the time to read about this journey. I hope you found some useful insights or inspiration in these pages. If you‚Äôre working on similar challenges, I‚Äôd love to hear about your experiences.\nAfter all, the best solutions often come from sharing knowledge and learning from each other.\n","wordCount":"1760","inLanguage":"en","datePublished":"2025-11-10T00:00:00Z","dateModified":"2025-11-10T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://defa-ekky1073.github.io/entry/performance-test-improvement/"},"publisher":{"@type":"Organization","name":"Ekky Defa Rizkyan","logo":{"@type":"ImageObject","url":"https://defa-ekky1073.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://defa-ekky1073.github.io/ accesskey=h title="Ekky Defa Rizkyan (Alt + H)">Ekky Defa Rizkyan</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://defa-ekky1073.github.io/ title=Home><span>Home</span></a></li><li><a href=https://defa-ekky1073.github.io/entry/ title=Blog><span>Blog</span></a></li><li><a href=https://defa-ekky1073.github.io/experience title=Experience><span>Experience</span></a></li><li><a href=https://defa-ekky1073.github.io/about title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Redefining Quipper‚Äôs Performance Testing Platform</h1><div class=post-meta><span title='2025-11-10 00:00:00 +0000 UTC'>November 10, 2025</span></div></header><div class=post-content><hr><h1 id=how-we-cut-execution-time-from-2-hours--15-minutes-and-saved-60kyear->How we cut execution time from 2 hours ‚Üí 15 minutes and saved $60K+/year üöÄ<a hidden class=anchor aria-hidden=true href=#how-we-cut-execution-time-from-2-hours--15-minutes-and-saved-60kyear->#</a></h1><p>Performance testing at scale is rarely glamorous. It‚Äôs slow, unpredictable, and often under-invested, until something breaks. When I joined Quipper, our performance-testing workflow was functional but far from efficient. Tests took nearly 2 hours to run, required significant engineering effort, and consumed more infrastructure than we were comfortable with. And as our products grew, the process was increasingly hard to justify.</p><hr><h2 id=the-problem>The problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h2><p>Before the redesign, our performance-testing workflow was slow, expensive, and almost entirely manual. Running a single test felt like assembling a lab experiment from scratch, every time.</p><p>Here‚Äôs how it worked:</p><p><img alt="Original Performance Testing Architecture" loading=lazy src=/assets/original-arch-diagram.webp title="Figure 1: Original performance testing architecture showing manual processes and dependencies"></p><h3 id=1-everything-was-manual>1. Everything was manual<a hidden class=anchor aria-hidden=true href=#1-everything-was-manual>#</a></h3><p>Nothing about the workflow was automated. Every test run required a human QA engineer to perform dozens of manual steps just to get things going.</p><h3 id=2-manual-file-preparation--upload>2. Manual file preparation + upload<a hidden class=anchor aria-hidden=true href=#2-manual-file-preparation--upload>#</a></h3><p>To start a test, the QA team had to manually prepare and upload all JMeter artifacts, such as:</p><ul><li>.jmx test plans</li><li>.csv dataset files</li><li>.test configuration assets</li></ul><p>These files were uploaded to S3, only to be manually pulled down again later on the servers where the test would run. This workflow was repetitive and time-consuming.</p><h3 id=3-overreliance-on-aws-workspaces-and-its-high-cost>3. Overreliance on AWS WorkSpaces (and its high cost)<a hidden class=anchor aria-hidden=true href=#3-overreliance-on-aws-workspaces-and-its-high-cost>#</a></h3><p>The entire test environment ran on AWS WorkSpaces, which is essentially a fully managed virtual desktop (VDI), great for office workers, not so great for distributed load testing.</p><p>For each performance test:</p><ul><li>QA engineers had to spin up 25 WorkSpace nodes</li><li>Each had to be started manually</li><li>And shut down manually</li></ul><p>Beyond the friction, WorkSpaces was overpriced for this use case.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>üìä Cost Estimation:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>A Windows WorkSpace (PowerPro bundle) is:
</span></span><span style=display:flex><span>‚Ä¢ $7.25 monthly base fee per instance
</span></span><span style=display:flex><span>‚Ä¢ $1.53/hour usage
</span></span><span style=display:flex><span>‚Ä¢ $1.53/hr √ó 25 instances √ó 2 hours/test = $76.50/run
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If QA runs the test once per day:
</span></span><span style=display:flex><span>‚Ä¢ $76.50 √ó 30 days = $2,295/month in usage fees
</span></span><span style=display:flex><span>‚Ä¢ Plus $7.25 √ó 25 instances = $181.25 in monthly base fees
</span></span><span style=display:flex><span>‚Ä¢ Total: ~$2,476.25/month
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If tests run twice per day:
</span></span><span style=display:flex><span>‚Ä¢ $153 √ó 30 days = $4,590/month in usage fees
</span></span><span style=display:flex><span>‚Ä¢ Plus $181.25 in monthly base fees
</span></span><span style=display:flex><span>‚Ä¢ Total: ~$4,771.25/month
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>That&#39;s only the compute cost ‚Äî storage, data transfer, and base fees add more.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Realistic total costs (including overhead):
</span></span><span style=display:flex><span>‚Ä¢ ~$6,000‚Äì$8,000/month
</span></span><span style=display:flex><span>‚Ä¢ Up to ~$72,000‚Äì$96,000/year
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>And that&#39;s only for performance testing.
</span></span></code></pre></div><p>Furthermore:</p><ul><li>WorkSpaces billing rounds hourly<ul><li>If you run 61 minutes, you pay 2 hours</li></ul></li><li>Billing confusion + manual shutdowns often led to over-billing</li><li>QA was limited to working within time windows to keep cost down</li></ul><p>In Short:</p><blockquote><p>‚ùó We were spending cloud-desktop money to run distributed load testing and paying for the privilege of doing it manually.</p></blockquote><h3 id=4-manual-distribution-of-test-artifacts>4. Manual distribution of test artifacts<a hidden class=anchor aria-hidden=true href=#4-manual-distribution-of-test-artifacts>#</a></h3><p>Once WorkSpaces were up, the QA engineer still had to:</p><ul><li>Connect to the first node using AWS Console</li><li>Download the test assets from S3</li><li>Run 15 separate Python files to distribute the test assets to each node</li></ul><blockquote><p>If anything failed, the QA had to restart the distribution manually. There was no retry automation, no integrity checking, and no self-healing.</p></blockquote><h3 id=5-manual-execution-of-jmeter>5. Manual execution of JMeter<a hidden class=anchor aria-hidden=true href=#5-manual-execution-of-jmeter>#</a></h3><p>After distribution, the QA would initiate JMeter from the first instance. This instance acted as the controller, coordinating 24 secondary nodes.</p><blockquote><p>Any misconfiguration, file mismatch, missing dependency, wrong path, etc., requires starting over.</p></blockquote><h3 id=6-manual-result-upload>6. Manual result upload<a hidden class=anchor aria-hidden=true href=#6-manual-result-upload>#</a></h3><p>Once the test finished:</p><ul><li>The QA exported results</li><li>Manually uploaded them back to S3</li><li>They have to download it manually to their local machine</li></ul><p>Only after that could anyone analyze the output or generate reports.</p><h3 id=7-manual-shutdown>7. Manual shutdown<a hidden class=anchor aria-hidden=true href=#7-manual-shutdown>#</a></h3><p><strong>Finally, and critically! All 25 WorkSpaces had to be shut down manually!</strong> If someone forgot, WorkSpaces kept charging by the hour. And because of hourly billing rounding, even short mistakes were costly.</p><blockquote><p>All of those steps took about 2 hours to complete. This meant:</p><ul><li>Long iteration cycles</li><li>Low test frequency</li><li>Higher risk of missed regressions</li><li>High operational burden</li></ul></blockquote><p>In short:</p><blockquote><p><em><strong>‚ùó üìâ Performance testing wasn‚Äôt a tool, it was a tax.</strong></em></p><p>It&rsquo;s wasting available resources: time, money, and man-hours.</p></blockquote><hr><h2 id=the-approach>The Approach<a hidden class=anchor aria-hidden=true href=#the-approach>#</a></h2><p>Once we identified that the real bottleneck wasn‚Äôt the testing tool, but the process, the goal became simple:</p><blockquote><p>Make performance testing fast, automated, flexible, and cost-efficient</p></blockquote><p>Instead of patching the old process, I decided to rebuild it from the ground up using Infrastructure as Code, automation, and lightweight orchestration.
<img alt="Finalized Performance Testing Architecture" loading=lazy src=/assets/final-arch-diagram.webp title="Figure 2: Finalized performance testing architecture"></p><h3 id=1-infrastructure-redesign-with-terraform--github-actions>1. Infrastructure Redesign with Terraform + GitHub Actions<a hidden class=anchor aria-hidden=true href=#1-infrastructure-redesign-with-terraform--github-actions>#</a></h3><p>The first step was to replace AWS WorkSpaces entirely. They were slow, expensive, and ill-suited for distributed automation.</p><p>I migrated the testing infrastructure to AWS EC2, provisioning all resources through Terraform. This gave us a clean, declarative setup where every environment could be built, destroyed, or modified automatically and repeatably.</p><p>To make the process fully automated and auditable:</p><ul><li>Terraform runs are executed directly through GitHub Actions</li><li>All infrastructure changes are committed and tracked in our IaC repository</li><li>Any updates to configuration automatically trigger deployments</li></ul><blockquote><p>Every test environment is now ephemeral: <strong>created on demand, and destroyed after use</strong>.</p></blockquote><p>Each EC2 instance is pre-configured automatically via user-data scripts, which install:</p><ul><li>JMeter</li><li>Dependencies (Python, monitoring agents, etc.)</li><li>Environment-specific configurations</li></ul><p>This means the QA engineer doesn‚Äôt need to install or configure anything manually, the infrastructure configures itself.</p><h3 id=2-simplifying-configuration-for-qa-engineers>2. Simplifying Configuration for QA Engineers<a hidden class=anchor aria-hidden=true href=#2-simplifying-configuration-for-qa-engineers>#</a></h3><p>While Terraform gave us automation, it wasn‚Äôt friendly to non-engineers. So I designed the setup around a single configuration file local.tf. which exposes all the tunable parameters the QA team might need:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-hcl data-lang=hcl><span style=display:flex><span><span style=color:#75715e>  # General Configuration
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  region            <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;us-east-1&#34;</span>
</span></span><span style=display:flex><span>  jmeter_version    <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;5.6.3&#34;</span>
</span></span><span style=display:flex><span>  bucket_name       <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;performance-test-bucket&#34;</span><span style=color:#75715e>           # Only change this if required, otherwise let it be
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  instance_tag      <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;performance_test&#34;</span><span style=color:#75715e>                  # This must be have same value as in python distributor, autimatically set worker and master tag
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  ami_id            <span style=color:#f92672>=</span> <span style=color:#66d9ef>data</span>.<span style=color:#66d9ef>aws_ami</span>.<span style=color:#66d9ef>amazon_linux_2023</span>.<span style=color:#66d9ef>id</span><span style=color:#75715e>   # Amazon Linux 2023 AMI (dynamically fetched)
</span></span></span><span style=display:flex><span><span style=color:#75715e>  # Change ami_id if you need to change instances OS, but make sure to update the user_data on instance.tf
</span></span></span><span style=display:flex><span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>  # Worker Configuration
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  instance_count       <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  worker_instance_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;c6g.large&#34;</span><span style=color:#75715e>   # You can check instance types on https://instances.vantage.sh/
</span></span></span><span style=display:flex><span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>  # Master Configuration
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  enable_master        <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span><span style=color:#75715e>          # Set this to false if you going to use your local machine as master
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  master_instance_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;c6g.large&#34;</span><span style=color:#75715e>   # You can check instance types on https://instances.vantage.sh/
</span></span></span></code></pre></div><p>All critical parameters, such as the number of nodes, instance type, tags, JMeter version can be adjusted from one place. QA engineers don‚Äôt have to touch multiple .tf files or understand Terraform internals.</p><blockquote><p>This abstraction turned infrastructure setup into something simple, safe, and self-serve.</p><p><strong>No SRE intervention needed</strong>.</p></blockquote><h3 id=3-introducing-pydist--the-python-controller>3. Introducing PyDist ‚Äî the Python Controller<a hidden class=anchor aria-hidden=true href=#3-introducing-pydist--the-python-controller>#</a></h3><p>Once the infrastructure was automated, I built a Python-based orchestration tool we call PyDist (Python Distributor) as the control layer that manages the entire testing lifecycle. PyDist connects everything into a single, reliable workflow.</p><p><img alt="Pydist - Python Distributor" loading=lazy src=/assets/pydist-screencap.png title="Figure 3: Pydist - Python Distributor"></p><p>Here‚Äôs what it does:</p><h4 id=1-configuration-validation>1. Configuration Validation<a hidden class=anchor aria-hidden=true href=#1-configuration-validation>#</a></h4><p>Utilizes Boto3 to verify that:</p><ul><li>All EC2 nodes (master + workers) are running and reachable</li><li>S3 buckets are accessible</li><li>Connections between master and worker nodes are healthy</li></ul><h4 id=2-file-distribution>2. File Distribution<a hidden class=anchor aria-hidden=true href=#2-file-distribution>#</a></h4><p>Automatically uploads all .jmx files, .csv variable files, and configurations to the appropriate nodes. No more manual S3 uploads or remote copy scripts.</p><h4 id=3-service-control>3. Service Control<a hidden class=anchor aria-hidden=true href=#3-service-control>#</a></h4><p>Starts and stops the JMeter service on each EC2 instance, ensuring consistent test environments every time.</p><h4 id=4-test-execution>4. Test Execution<a hidden class=anchor aria-hidden=true href=#4-test-execution>#</a></h4><p>Runs CLI-based JMeter tests from the master node, coordinating all worker nodes in distributed mode. PyDist also monitors progress and collects test logs in real-time.</p><h4 id=5-result-management>5. Result Management<a hidden class=anchor aria-hidden=true href=#5-result-management>#</a></h4><p>Once tests complete:</p><ul><li>Results are automatically uploaded to S3</li><li>PyDist downloads the results to the local machine for analysis</li><li>It can also re-download past test results if needed for reporting or regression comparison</li></ul><p>In essence, PyDist replaced dozens of manual steps with a single command and brought order, visibility, and repeatability to the whole process.</p><h3 id=4-automating-cost-efficiency>4. Automating Cost Efficiency<a hidden class=anchor aria-hidden=true href=#4-automating-cost-efficiency>#</a></h3><p>With the infrastructure and orchestration layers complete, there was one more challenge: <strong>idle, unused resources</strong>.</p><p>To prevent unnecessary cloud costs, I created two GitHub Actions workflows:</p><h4 id=1-auto-shutdown-workflow>1. Auto-Shutdown Workflow<a hidden class=anchor aria-hidden=true href=#1-auto-shutdown-workflow>#</a></h4><ul><li>Runs every midnight</li><li>Automatically shuts down all EC2 nodes tagged as performance-testing instances</li><li>Ensures we never pay for unused instances, even if someone forgets to stop them</li></ul><h4 id=2-cluster-startup-workflow>2. Cluster Startup Workflow<a hidden class=anchor aria-hidden=true href=#2-cluster-startup-workflow>#</a></h4><ul><li>Triggered manually when QA needs to run tests</li><li>Spins up the entire test cluster automatically</li><li>QA no longer needs AWS console access or Terraform knowledge, it‚Äôs just a button click</li></ul><p>This combination gave us full control, safety, and significant cost savings without sacrificing flexibility.</p><h3 id=5-the-outcome>5. The Outcome<a hidden class=anchor aria-hidden=true href=#5-the-outcome>#</a></h3><p>By combining Terraform, GitHub Actions, and PyDist, we turned a clunky, manual workflow into a streamlined, automated, and maintainable system.</p><ul><li>‚úÖ No more manual uploads or remote logins</li><li>‚úÖ Fully documented and version-controlled infrastructure</li><li>‚úÖ Self-service configuration for QA</li><li>‚úÖ Automated start/stop + cost control</li><li>‚úÖ Reliable, repeatable, fast test runs</li></ul><p>And the result?</p><ul><li>üöÄ Test cycle time reduced from 2 hours ‚Üí 15 minutes</li><li>üí∏ ~$60,000/year in infrastructure savings</li><li>üòÉ Plus unmeasured (but huge) savings in time, manual effort, and frustration.</li></ul><hr><h2 id=lesson-learned>Lesson Learned<a hidden class=anchor aria-hidden=true href=#lesson-learned>#</a></h2><p>Rebuilding the performance-testing platform taught me that real reliability isn‚Äôt about tools, it‚Äôs about systems. The old workflow didn‚Äôt fail because JMeter was bad; it failed because the process around it wasn‚Äôt built to scale.</p><p>Here‚Äôs what stood out the most:</p><h3 id=1-fix-the-process-not-just-the-tool>1. Fix the process, not just the tool<a hidden class=anchor aria-hidden=true href=#1-fix-the-process-not-just-the-tool>#</a></h3><p>Automation works best when it removes human dependency. By redesigning the workflow we turned a manual process into a repeatable system.</p><h3 id=2-iac-is-communication-not-just-code>2. IaC is communication, not just code<a hidden class=anchor aria-hidden=true href=#2-iac-is-communication-not-just-code>#</a></h3><p>Terraform made every change visible, reviewable, and reversible. It replaced tribal knowledge with a shared, documented truth.</p><h3 id=3-empower-dont-gatekeep>3. Empower, don‚Äôt gatekeep<a hidden class=anchor aria-hidden=true href=#3-empower-dont-gatekeep>#</a></h3><p>By simplifying configuration for QA, we gave ownership back to the team. They could run, tweak, and iterate without waiting on SREs.</p><h3 id=4-build-cost-efficiency-into-design>4. Build cost efficiency into design<a hidden class=anchor aria-hidden=true href=#4-build-cost-efficiency-into-design>#</a></h3><p>We stopped relying on people to remember to shut things down. With automation, cost-saving became an inherent behavior, not an afterthought.</p><h3 id=5-make-testing-a-habit>5. Make testing a habit<a hidden class=anchor aria-hidden=true href=#5-make-testing-a-habit>#</a></h3><p>When tests are fast and simple, people run them often. That‚Äôs how reliability becomes part of the culture, not a special event.</p><hr><h2 id=final-take>Final Take<a hidden class=anchor aria-hidden=true href=#final-take>#</a></h2><p>What started as a slow, frustrating process became an elegant system that just works. Tests that once took hours now finish in minutes, infrastructure spins up and down on demand, and engineers can focus on insight instead of setup.</p><p>But the biggest win isn‚Äôt the speed or the savings: <strong>it‚Äôs the shift in mindset</strong>. Performance testing stopped being a chore and became a shared discipline.</p><p>Thank you for taking the time to read about this journey. I hope you found some useful insights or inspiration in these pages. If you&rsquo;re working on similar challenges, I&rsquo;d love to hear about your experiences.</p><blockquote><p>After all, the best solutions often come from sharing knowledge and learning from each other.</p></blockquote></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://defa-ekky1073.github.io/>Ekky Defa Rizkyan</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>